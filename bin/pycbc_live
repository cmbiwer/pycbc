#!/usr/bin/env python
import multiprocessing, argparse, numpy, pycbc, logging, cProfile
from pycbc import version, waveform, types, filter as pfilter, scheme, psd as pypsd, noise, vetoes
from pycbc.types import MultiDetOptionAction, zeros

from pycbc.fft import ifft, IFFT
from pycbc.filter import correlate
from pycbc import strain
import pycbc.events

parser = argparse.ArgumentParser()
parser.add_argument('--verbose', action='store_true')
parser.add_argument('--version', action='version', version=version.git_verbose_msg)
parser.add_argument('--bank-file', help="Template bank xml file")
parser.add_argument('--low-frequency-cutoff', help="low frequency cutoff", type=int)
parser.add_argument('--sample-rate', help="output sample rate", type=int)
parser.add_argument('--chisq-bins', help="Number of chisq bins")
parser.add_argument('--analysis-chunk', type=int,
                        help="Amount of data to produce triggers in a  block")
parser.add_argument('--snr-threshold', type=float)
parser.add_argument('--channel-name', action=MultiDetOptionAction, nargs='+')
parser.add_argument('--frame-src', action=MultiDetOptionAction, nargs='+')
parser.add_argument('--highpass-frequency', type=float,
                        help="Frequency to apply highpass filtering")
parser.add_argument('--highpass-reduction', type=float,
                        help="DB to reduce low frequencies")
parser.add_argument('--highpass-bandwidth', type=float,
                        help="Width of the highpass turnover region in Hz")
parser.add_argument('--psd-recalculate-every', type=int, default=1)
parser.add_argument('--psd-samples', type=int, 
                        help="Number of PSD segments to use in the rolling estimate")
parser.add_argument('--psd-segment-length', type=int, 
                        help="Length in seconds of each PSD segment")
parser.add_argument('--psd-inverse-length', type=float, 
                        help="Lenght in time for the equivelant FIR filter")
parser.add_argument('--start-time', type=int, default=0)
parser.add_argument('--approximant')
scheme.insert_processing_option_group(parser)

args = parser.parse_args()
scheme.verify_processing_options(args, parser)
pycbc.init_logging(args.verbose)

ctx = scheme.from_cli(args)

sr = args.sample_rate
flow = args.low_frequency_cutoff

# Approximant guess of the total padding
valid_pad = args.analysis_chunk
total_pad = args.psd_segment_length * 2 + valid_pad
bank = waveform.LiveFilterBank(args.bank_file, flow, sr, total_pad,
                               approximant=args.approximant)

waveforms = list(bank)
lengths = numpy.array([1.0 / waveform.delta_f for waveform in waveforms])

ifos = args.channel_name.keys()

data_reader = {}
for ifo in ifos:
    data_reader[ifo] = pycbc.strain.StrainBuffer([args.frame_src[ifo]], 
                        '%s:%s' % (ifo, args.channel_name[ifo]),
                        args.start_time, 
                        max_buffer=lengths.max(),
                        sample_rate=args.sample_rate,
                        highpass_frequency=args.highpass_frequency,
                        highpass_reduction=args.highpass_reduction,
                        highpass_bandwidth=args.highpass_bandwidth,
                        psd_samples=args.psd_samples,
                        psd_segment_length=args.psd_segment_length,
                        psd_inverse_length=args.psd_inverse_length)

class BatchMatchedFilter(object):
    def __init__(self, templates, maxelements=2**22):
        durations = numpy.array([1.0 / t.delta_f for t in templates])

        lsort = durations.argsort()
        durations = durations[lsort]
        templates = [templates[li] for li in lsort]

        # Figure out how to chunk together the templates into groups to process
        sizes, counts = numpy.unique(durations, return_counts=True)
        tsamples = [(len(t) - 1) * 2 for t in templates]
        grabs = maxelements / numpy.unique(tsamples) 

        chunks = numpy.array([])
        num = 0
        for count, grab in zip(counts, grabs):
            chunks = numpy.append(chunks, numpy.arange(num, count + num, grab))
            chunks = numpy.append(chunks, [count + num])
            num += count
        chunks = numpy.unique(chunks).astype(numpy.uint32)

        # We now have how many templates to grab at a time.
        self.chunks = chunks[1:] - chunks[0:-1]

        self.out_mem = {}
        self.cout_mem = {}
        self.ifts = {}
        chunk_durations = [durations[i] for i in chunks[:-1]]
        self.chunk_tsamples = [tsamples[int(i)] for i in chunks[:-1]]
        samples = self.chunk_tsamples * self.chunks
 
        # Create workspace memory for correlate and snr      
        mem_ids = [(a, b) for a, b in zip(chunk_durations, self.chunks)]
        mem_types = set(zip(mem_ids, samples))

        self.tgroups, self.mids = [], []
        for i, size in mem_types:
            dur, count = i
            self.out_mem[i] = zeros(size, dtype=numpy.complex64)
            self.cout_mem[i] = zeros(size, dtype=numpy.complex64)
            self.ifts[i] = IFFT(self.cout_mem[i], self.out_mem[i],
                                nbatch=count,
                                size=len(self.cout_mem[i]) / count)

        # Split the templates into their processing groups
        for dur, count in mem_ids:
            tgroup = templates[0:count]
            self.tgroups.append(tgroup)
            self.mids.append((dur, count))
            templates = templates[count:]

        # Associate the snr and corr memory block to each template
        for i, tgroup in enumerate(self.tgroups):
            psize = self.chunk_tsamples[i]
            s = 0
            e = psize
            mid = self.mids[i]
            for htilde in tgroup:
                htilde.out = self.out_mem[mid][s:e]
                htilde.cout = self.cout_mem[mid][s:e]
                s += psize
                e += psize

    def set_data(self, data):
        self.data = data
        self.block_id = 0

    def process_batch(self):       
        if self.block_id == len(self.tgroups):
            return None

        tgroup = self.tgroups[self.block_id]
        psize = self.chunk_tsamples[self.block_id]
        mid = self.mids[self.block_id]
        stilde = self.data.overwhitened_data(tgroup[0].delta_f)
        psd = stilde.psd 

        valid_end = psize - self.data.total_corruption
        valid_start = int(valid_end - valid_pad * args.sample_rate)
        seg = slice(valid_start, valid_end)

        for htilde in tgroup:
            correlate(htilde, stilde, htilde.cout[0:len(stilde)])

        self.ifts[mid].execute()

        snr = numpy.zeros(len(tgroup), dtype=numpy.complex64)
        chisq = numpy.zeros(len(tgroup), dtype=numpy.float32)

        for i, htilde in enumerate(tgroup):
            # Find peaks
            m, l = htilde.out[seg].abs_max_loc()
            #m, l = pycbc.events.threshold_and_cluster(htilde.out[seg],
            #                     args.snr_threshold,
            #                     len(htilde.out[seg]))

            #exit()            
            l += valid_start

            # If nothing is above threshold we can exit this template
            norm = 4.0 * htilde.delta_f / (htilde.sigmasq(psd) ** 0.5)
            if m * norm < args.snr_threshold:
                continue    
     
            # calculate chisq
            snrv = numpy.array([htilde.out[l]])
            chisq[i], dof = power_chisq.values(htilde.cout, snrv, norm, psd, [l], htilde)

            chisq[i] /= dof
            snr[i] = snrv[0] * norm
            
        self.block_id += 1
        return snr, chisq


# get more data
power_chisq = vetoes.SingleDetPowerChisq(args.chisq_bins, None)

with ctx:
    mf = BatchMatchedFilter(waveforms)

    # prime pump for testing
    for i in range(35):
        for ifo in ifos:
            data_reader[ifo].advance(valid_pad)
    for ifo in ifos:
        data_reader[ifo].recalculate_psd()

    pr = cProfile.Profile()
    pr.enable()

    #while 1:
    for i in range(20):
        logging.info('Conditioning Data')
        for ifo in ifos:
            data_reader[ifo].advance(valid_pad)

            if i % args.psd_recalculate_every == 0:
                data_reader[ifo].recalculate_psd()     
 
            mf.set_data(data_reader[ifo])
            
            logging.info('Filtering: %s' % ifo)
        
            while 1:
                result = mf.process_batch()     
                if result is None: break
                
                s, c = result

pr.dump_stats('log')       


# cluster coincs over bank, calculate FAR

# send report if above threshold


pr.disable()
pr.dump_stats('log')
