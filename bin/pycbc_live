#!/usr/bin/env python
import multiprocessing, argparse, numpy, pycbc, logging, cProfile
from pycbc import version, waveform, types, filter as pfilter, scheme, psd as pypsd, noise, vetoes
from pycbc.types import MultiDetOptionAction, zeros

from pycbc.fft import ifft
from pycbc.filter import correlate
from pycbc import strain

parser = argparse.ArgumentParser()
parser.add_argument('--verbose', action='store_true')
parser.add_argument('--version', action='version', version=version.git_verbose_msg)
parser.add_argument('--bank-file', help="Template bank xml file")
parser.add_argument('--low-frequency-cutoff', help="low frequency cutoff", type=int)
parser.add_argument('--sample-rate', help="output sample rate", type=int)
parser.add_argument('--chisq-bins', help="Number of chisq bins")
parser.add_argument('--analysis-chunk', type=int,
                        help="Amount of data to produce triggers in a  block")
parser.add_argument('--snr-threshold', type=float)
parser.add_argument('--channel-name', action=MultiDetOptionAction, nargs='+')
parser.add_argument('--frame-src', action=MultiDetOptionAction, nargs='+')
parser.add_argument('--highpass-frequency', type=float,
                        help="Frequency to apply highpass filtering")
parser.add_argument('--highpass-reduction', type=float,
                        help="DB to reduce low frequencies")
parser.add_argument('--highpass-bandwidth', type=float,
                        help="Width of the highpass turnover region in Hz")
parser.add_argument('--psd-recalculate-every', type=int, default=1)
parser.add_argument('--psd-samples', type=int, 
                        help="Number of PSD segments to use in the rolling estimate")
parser.add_argument('--psd-segment-length', type=int, 
                        help="Length in seconds of each PSD segment")
parser.add_argument('--psd-inverse-length', type=float, 
                        help="Lenght in time for the equivelant FIR filter")
parser.add_argument('--start-time', type=int, default=0)

parser.add_argument('--approximant')
scheme.insert_processing_option_group(parser)

args = parser.parse_args()
scheme.verify_processing_options(args, parser)
pycbc.init_logging(args.verbose)

ctx = scheme.from_cli(args)

sr = args.sample_rate
flow = args.low_frequency_cutoff

# Approximant guess of the total padding
valid_pad = args.analysis_chunk
total_pad = args.psd_segment_length * 2 + valid_pad
bank = waveform.LiveFilterBank(args.bank_file, flow, sr, total_pad,
                               approximant=args.approximant)

waveforms = list(bank)
lengths = numpy.array([1.0 / waveform.delta_f for waveform in waveforms])

ifos = args.channel_name.keys()

data_reader = {}
for ifo in ifos:
    data_reader[ifo] = pycbc.strain.StrainBuffer([args.frame_src[ifo]], 
                        '%s:%s' % (ifo, args.channel_name[ifo]),
                        args.start_time, 
                        max_buffer=lengths.max(),
                        sample_rate=args.sample_rate,
                        highpass_frequency=args.highpass_frequency,
                        highpass_reduction=args.highpass_reduction,
                        highpass_bandwidth=args.highpass_bandwidth,
                        psd_samples=args.psd_samples,
                        psd_segment_length=args.psd_segment_length,
                        psd_inverse_length=args.psd_inverse_length)

class BatchMatchedFilter(object):
    def __init__(self, templates, maxelements=2**23):
        durations = numpy.array([1.0 / t.delta_f for t in templates])

        lsort = durations.argsort()
        durations = durations[lsort]
        templates = [templates[li] for li in lsort]

        # Figure out how to chunk together the templates into groups to process
        sizes, counts = numpy.unique(durations, return_counts=True)
        print counts
        tsamples = [(len(t) - 1) * 2 for t in templates]
        grabs = maxelements / numpy.unique(tsamples) 

        chunks = numpy.array([])
        num = 0
        for count, grab in zip(counts, grabs):
            chunks = numpy.append(chunks, numpy.arange(num, count + num, grab))
            chunks = numpy.append(chunks, [count + num])
            num += count
        chunks = numpy.unique(chunks).astype(numpy.uint32)

        # We now have how many templates to grab at a time.
        self.chunks = chunks[1:] - chunks[0:-1]

        self.out_mem = {}
        self.cout_mem = {}
        chunk_durations = [durations[i] for i in chunks[:-1]]
        self.chunk_tsamples = [tsamples[int(i)] for i in chunks[:-1]]
        samples = self.chunk_tsamples * self.chunks
 
        # Create workspace memory for correlate and snr      
        mem_ids = [(a, b) for a, b in zip(chunk_durations, self.chunks)]
        mem_types = set(zip(mem_ids, samples))

        self.tgroups, self.mids = [], []
        for i, size in mem_types:
            self.out_mem[i] = zeros(size, dtype=numpy.complex64)
            self.cout_mem[i] = zeros(size, dtype=numpy.complex64)

        # Split the templates into their processing groups
        for dur, count in mem_ids:
            tgroup = templates[0:count]
            self.tgroups.append(tgroup)
            self.mids.append((dur, count))
            templates = templates[count:]

    def set_data(self, data):
        self.data = data
        self.block_id = 0

    def process_batch(self):
        if self.block_id == len(self.tgroups):
            return None

        print numpy.array([len(t) for t in self.tgroups]).sum()

        tgroup = self.tgroups[self.block_id]
        psize = self.chunk_tsamples[self.block_id]
        mid = self.mids[self.block_id]

        s = 0
        e = psize

        for htilde in tgroup:
            stilde = self.data.overwhitened_data(htilde.delta_f)
            psd = stilde.psd   

            out = self.out_mem[mid][s:e]
            cout = self.cout_mem[mid][s:e]

            # Matched Filter
            norm = 4.0 * htilde.delta_f / (htilde.sigmasq(psd) ** 0.5)
            correlate(htilde, stilde, cout[0:len(stilde)])
            ifft(cout, out)

            # Find peaks
            valid_end = len(out) - data_reader[ifo].total_corruption
            valid_start = int(valid_end - valid_pad * args.sample_rate)
            seg = slice(valid_start, valid_end)

            m, l = out[seg].abs_max_loc()
            l += valid_start

            # If nothing is above threshold we can exit this template
            if m * norm < args.snr_threshold:
                continue    
     
            # calculate chisq
            snrv = numpy.array([out[l]])
            chisq, dof = power_chisq.values(cout, snrv, norm, psd, [l], htilde)
            chisq /= dof

            s += psize
            e += psize

        self.block_id += 1
        return True


mf = BatchMatchedFilter(waveforms)

pr = cProfile.Profile()
pr.enable()

# get more data

power_chisq = vetoes.SingleDetPowerChisq(args.chisq_bins, None)


with ctx:
    # prime pump for testing
    for i in range(35):
        for ifo in ifos:
            data_reader[ifo].advance(valid_pad)
    for ifo in ifos:
        data_reader[ifo].recalculate_psd()

    for i in range(10):
        logging.info('Conditioning Data')
        for ifo in ifos:
            data_reader[ifo].advance(valid_pad)

            if i % args.psd_recalculate_every == 0:
                data_reader[ifo].recalculate_psd()     
 
            mf.set_data(data_reader[ifo])
            
            logging.info('Start Filtering: %s' % ifo)
            while mf.process_batch() is not None:
                pass

            logging.info('End Filtering')

# cluster coincs over bank, calculate FAR

# send report if above threshold


pr.disable()
pr.dump_stats('log')
